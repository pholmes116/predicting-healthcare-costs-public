{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"id": "WN6uDGMhpXIw", "tags": []}, "outputs": [], "source": "import os\nimport pandas as pd\nimport seaborn as sns"}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "AcARmI8QpXGO", "outputId": "731b4251-775e-4758-a0e7-98ab111ad9a5", "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip install pyspark"}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "y3mqRk7RpXDd", "tags": []}, "outputs": [], "source": "# standard libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# PySpark libraries\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as sql_f\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import to_date, datediff, floor, col, avg, substring\n\n# DL/ML libraries\nfrom sklearn.metrics import accuracy_score\nspark = SparkSession.builder.getOrCreate()\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "nH7CQZgqUGTg", "tags": []}, "outputs": [], "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.functions import unix_timestamp"}, {"cell_type": "markdown", "metadata": {"id": "tNKp8qk0WPxF"}, "source": "for working in GCP"}, {"cell_type": "markdown", "metadata": {}, "source": "## 1) Loading the data"}, {"cell_type": "code", "execution_count": 11, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u2705 Synthea downloaded\nSLF4J: No SLF4J providers were found.\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#noProviders for further details.\nScanned 88 modules and 152 submodules.\nLoading submodule modules/allergies/allergy_panel.json\nLoading submodule modules/allergies/drug_allergy_incidence.json\nLoading submodule modules/allergies/environmental_allergy_incidence.json\nLoading submodule modules/allergies/food_allergy_incidence.json\nLoading submodule modules/allergies/immunotherapy.json\nLoading submodule modules/allergies/outgrow_env_allergies.json\nLoading submodule modules/allergies/outgrow_food_allergies.json\nLoading submodule modules/allergies/severe_allergic_reaction.json\nLoading submodule modules/anemia/anemia_sub.json\nLoading submodule modules/breast_cancer/chemotherapy_breast.json\nLoading submodule modules/breast_cancer/hormone_diagnosis.json\nLoading submodule modules/breast_cancer/hormonetherapy_breast.json\nLoading submodule modules/breast_cancer/surgery_therapy_breast.json\nLoading submodule modules/breast_cancer/tnm_diagnosis.json\nLoading submodule modules/contraceptives/clear_contraceptive.json\nLoading submodule modules/contraceptives/female_sterilization.json\nLoading submodule modules/contraceptives/implant_contraceptive.json\nLoading submodule modules/contraceptives/injectable_contraceptive.json\nLoading submodule modules/contraceptives/intrauterine_device.json\nLoading submodule modules/contraceptives/male_sterilization.json\nLoading submodule modules/contraceptives/oral_contraceptive.json\nLoading submodule modules/contraceptives/patch_contraceptive.json\nLoading submodule modules/contraceptives/ring_contraceptive.json\nLoading submodule modules/covid19/admission.json\nLoading submodule modules/covid19/determine_risk.json\nLoading Lookup Table: covid-19-severity-outcomes.csv\nLoading Lookup Table: covid-19-survival-outcomes.csv\nLoading submodule modules/covid19/diagnose_bacterial_infection.json\nLoading submodule modules/covid19/diagnose_blood_clot.json\nLoading submodule modules/covid19/end_outcomes.json\nLoading submodule modules/covid19/end_symptoms.json\nLoading submodule modules/covid19/infection.json\nLoading submodule modules/covid19/measurements_daily.json\nLoading submodule modules/covid19/measurements_frequent.json\nLoading submodule modules/covid19/measurements_vitals.json\nLoading submodule modules/covid19/medications.json\nLoading submodule modules/covid19/nonsurvivor_lab_values.json\nLoading submodule modules/covid19/outcomes.json\nLoading submodule modules/covid19/supplies_hospitalization.json\nLoading submodule modules/covid19/supplies_icu.json\nLoading submodule modules/covid19/supplies_intubation.json\nLoading submodule modules/covid19/survivor_lab_values.json\nLoading submodule modules/covid19/symptoms.json\nLoading submodule modules/covid19/treat_blood_clot.json\nLoading submodule modules/dermatitis/early_moderate_eczema_obs.json\nLoading submodule modules/dermatitis/early_severe_eczema_obs.json\nLoading submodule modules/dermatitis/mid_moderate_eczema_obs.json\nLoading submodule modules/dermatitis/mid_severe_eczema_obs.json\nLoading submodule modules/dermatitis/moderate_cd_obs.json\nLoading submodule modules/dermatitis/severe_cd_obs.json\nLoading submodule modules/dme/wheelchair.json\nLoading submodule modules/dme/wheelchair_end.json\nLoading submodule modules/encounter/anxiety_screening.json\nLoading submodule modules/encounter/depression_screening.json\nLoading submodule modules/encounter/fall_risk_screening.json\nLoading submodule modules/encounter/hark_screening.json\nLoading submodule modules/encounter/hospital_basic_labs.json\nLoading submodule modules/encounter/sdoh_hrsn.json\nLoading submodule modules/encounter/substance_use_screening.json\nLoading submodule modules/encounter/vitals.json\nLoading submodule modules/heart/acs_anticoagulant.json\nLoading submodule modules/heart/acs_antiplatelet.json\nLoading submodule modules/heart/acs_arrival_medications.json\nLoading submodule modules/heart/acs_discharge_meds.json\nLoading submodule modules/heart/avrr/antithrombotic.json\nLoading submodule modules/heart/avrr/avrr_referral.json\nLoading submodule modules/heart/avrr/intraop_meds_blood.json\nLoading submodule modules/heart/avrr/outcomes.json\nLoading submodule modules/heart/avrr/preoperative.json\nLoading submodule modules/heart/avrr/savrr_operation.json\nLoading submodule modules/heart/avrr/savrr_postop.json\nLoading submodule modules/heart/avrr/sequence.json\nLoading submodule modules/heart/cabg/cabg_referral.json\nLoading submodule modules/heart/cabg/details.json\nLoading Lookup Table: cabg_details_operative_approach.csv\nLoading Lookup Table: cabg_details_num_grafts.csv\nLoading Lookup Table: cabg_details_num_art_cond.csv\nLoading submodule modules/heart/cabg/icu_meds_devices.json\nLoading submodule modules/heart/cabg/labs_common.json\nLoading submodule modules/heart/cabg/operation.json\nLoading submodule modules/heart/cabg/or_intraop.json\nLoading submodule modules/heart/cabg/or_labs_meds.json\nLoading submodule modules/heart/cabg/outcomes.json\nLoading submodule modules/heart/cabg/postop.json\nLoading submodule modules/heart/cabg/postop_blood.json\nLoading submodule modules/heart/cabg/preoperative.json\nLoading submodule modules/heart/cabg_sequence.json\nLoading submodule modules/heart/cardiac_labs.json\nLoading submodule modules/heart/chf_lab_work.json\nLoading submodule modules/heart/chf_lvad.json\nLoading submodule modules/heart/chf_meds.json\nLoading submodule modules/heart/chf_meds_hfmef.json\nLoading submodule modules/heart/chf_meds_hfref_nyha2.json\nLoading submodule modules/heart/chf_meds_hfref_nyha3.json\nLoading submodule modules/heart/chf_meds_hfref_nyha4.json\nLoading submodule modules/heart/chf_nyha_panel.json\nLoading submodule modules/heart/chf_transplant.json\nLoading submodule modules/heart/nsteacs_pathway.json\nLoading submodule modules/heart/operative_status.json\nLoading submodule modules/heart/or_blood.json\nLoading Lookup Table: or_blood_anemia_check.csv\nLoading Lookup Table: or_blood_platelet_check.csv\nLoading Lookup Table: or_blood_plasma_check.csv\nLoading submodule modules/heart/savrepair/operative_status.json\nLoading submodule modules/heart/savreplace/operative_status.json\nLoading submodule modules/heart/stemi_fibrinolytic.json\nLoading submodule modules/heart/stemi_pathway.json\nLoading submodule modules/heart/tavr/alt_access.json\nLoading submodule modules/heart/tavr/operation.json\nLoading submodule modules/heart/tavr/operative_status.json\nLoading submodule modules/heart/tavr/outcomes.json\nLoading submodule modules/heart/tavr/postop.json\nLoading submodule modules/heart/vhd_risks.json\nLoading submodule modules/hiv/art_sequence.json\nLoading submodule modules/hiv/art_sequence_1987_1994.json\nLoading submodule modules/hiv/art_sequence_1995_1996.json\nLoading submodule modules/hiv/art_sequence_1997_2002.json\nLoading submodule modules/hiv/art_sequence_2003_2005.json\nLoading submodule modules/hiv/art_sequence_2006_2014.json\nLoading submodule modules/hiv/art_sequence_2015.json\nLoading submodule modules/hiv/hiv_baseline.json\nLoading submodule modules/hiv/hiv_cd4.json\nLoading Lookup Table: hiv_stage.csv\nLoading submodule modules/hiv/hiv_oi_prophylaxis.json\nLoading submodule modules/hiv/hiv_screening.json\nLoading submodule modules/hiv/hiv_viral_load.json\nLoading submodule modules/hiv/stop_all_art_meds.json\nLoading submodule modules/injuries/broken_jaw.json\nLoading submodule modules/lung_cancer/lung_cancer_probabilities.json\nLoading submodule modules/medications/ace_arb.json\nLoading Lookup Table: ace_arb_ingredient_distribution.csv\nLoading Lookup Table: ace_arb_amlodipine_benazepril_product_distribution.csv\nLoading Lookup Table: ace_arb_benazepril_product_distribution.csv\nLoading Lookup Table: ace_arb_benazepril_hydrochlorothiazide_product_distribution.csv\nLoading Lookup Table: ace_arb_enalapril_product_distribution.csv\nLoading Lookup Table: ace_arb_hydrochlorothiazide_lisinopril_product_distribution.csv\nLoading Lookup Table: ace_arb_hydrochlorothiazide_losartan_product_distribution.csv\nLoading Lookup Table: ace_arb_hydrochlorothiazide_valsartan_product_distribution.csv\nLoading Lookup Table: ace_arb_irbesartan_product_distribution.csv\nLoading Lookup Table: ace_arb_lisinopril_product_distribution.csv\nLoading Lookup Table: ace_arb_losartan_product_distribution.csv\nLoading Lookup Table: ace_arb_quinapril_product_distribution.csv\nLoading Lookup Table: ace_arb_ramipril_product_distribution.csv\nLoading Lookup Table: ace_arb_sacubitril_valsartan_product_distribution.csv\nLoading Lookup Table: ace_arb_telmisartan_product_distribution.csv\nLoading Lookup Table: ace_arb_valsartan_product_distribution.csv\nLoading submodule modules/medications/beta_blocker.json\nLoading Lookup Table: beta_blocker_ingredient_distribution.csv\nLoading Lookup Table: beta_blocker_atenolol_product_distribution.csv\nLoading Lookup Table: beta_blocker_bisoprolol_product_distribution.csv\nLoading Lookup Table: beta_blocker_bisoprolol_hydrochlorothiazide_product_distribution.csv\nLoading Lookup Table: beta_blocker_carvedilol_product_distribution.csv\nLoading Lookup Table: beta_blocker_labetalol_product_distribution.csv\nLoading Lookup Table: beta_blocker_metoprolol_product_distribution.csv\nLoading Lookup Table: beta_blocker_nebivolol_product_distribution.csv\nLoading Lookup Table: beta_blocker_propranolol_product_distribution.csv\nLoading submodule modules/medications/ear_infection_antibiotic.json\nLoading submodule modules/medications/emergency_inhaler.json\nLoading Lookup Table: emergency_inhaler_ingredient_distribution.csv\nLoading Lookup Table: emergency_inhaler_albuterol_product_distribution.csv\nLoading Lookup Table: emergency_inhaler_levalbuterol_product_distribution.csv\nLoading submodule modules/medications/hypertension_medication.json\nLoading submodule modules/medications/maintenance_inhaler.json\nLoading Lookup Table: maintenance_inhaler_ingredient_distribution.csv\nLoading Lookup Table: maintenance_inhaler_beclomethasone_product_distribution.csv\nLoading Lookup Table: maintenance_inhaler_budesonide_product_distribution.csv\nLoading Lookup Table: maintenance_inhaler_fluticasone_product_distribution.csv\nLoading Lookup Table: maintenance_inhaler_mometasone_product_distribution.csv\nLoading submodule modules/medications/moderate_opioid_pain_reliever.json\nLoading submodule modules/medications/otc_antihistamine.json\nLoading submodule modules/medications/otc_pain_reliever.json\nLoading submodule modules/medications/statin.json\nLoading Lookup Table: statin_ingredient_distribution.csv\nLoading Lookup Table: statin_atorvastatin_product_distribution.csv\nLoading Lookup Table: statin_ezetimibe_simvastatin_product_distribution.csv\nLoading Lookup Table: statin_lovastatin_product_distribution.csv\nLoading Lookup Table: statin_pitavastatin_product_distribution.csv\nLoading Lookup Table: statin_pravastatin_product_distribution.csv\nLoading Lookup Table: statin_rosuvastatin_product_distribution.csv\nLoading Lookup Table: statin_simvastatin_product_distribution.csv\nLoading submodule modules/medications/strong_opioid_pain_reliever.json\nLoading submodule modules/metabolic_syndrome/amputations.json\nLoading submodule modules/metabolic_syndrome/dme_supplies.json\nLoading submodule modules/metabolic_syndrome/eye_conditions.json\nLoading submodule modules/metabolic_syndrome/kidney_conditions.json\nLoading submodule modules/metabolic_syndrome/medications.json\nLoading submodule modules/snf/skilled_nursing_facility.json\nLoading submodule modules/surgery/general_anesthesia.json\nLoading submodule modules/total_joint_replacement/functional_status_assessments.json\nLoading submodule modules/uti/abx_tx.json\nLoading submodule modules/uti/ambulatory_eval.json\nLoading submodule modules/uti/ambulatory_path.json\nLoading submodule modules/uti/ed_bundle.json\nLoading submodule modules/uti/ed_eval.json\nLoading submodule modules/uti/ed_path.json\nLoading submodule modules/uti/gu_pregnancy_check.json\nLoading submodule modules/uti/hpi.json\nLoading submodule modules/uti/lab_follow_up.json\nLoading submodule modules/uti/labs.json\nLoading submodule modules/uti/telemed_path.json\nLoading submodule modules/veterans/veteran_suicide_probabilities.json\nLoading submodule modules/weight_loss/mend_week.json\nLoading module modules/acute_myeloid_leukemia.json\nLoading Lookup Table: AML.csv\nLoading module modules/allergic_rhinitis.json\nLoading module modules/allergies.json\nLoading module modules/anemia___unknown_etiology.json\nLoading module modules/appendicitis.json\nLoading module modules/asthma.json\nLoading module modules/atopy.json\nLoading module modules/atrial_fibrillation.json\nLoading module modules/attention_deficit_disorder.json\nLoading module modules/bone_marrow_transplant.json\nLoading module modules/breast_cancer.json\nLoading module modules/bronchitis.json\nLoading module modules/cerebral_palsy.json\nLoading module modules/chronic_kidney_disease.json\nLoading module modules/colorectal_cancer.json\nLoading module modules/congestive_heart_failure.json\nLoading module modules/contraceptive_maintenance.json\nLoading module modules/contraceptives.json\nLoading module modules/copd.json\nLoading module modules/covid19.json\nLoading Lookup Table: covid19_prob.csv\nLoading module modules/cystic_fibrosis.json\nLoading module modules/dementia.json\nLoading module modules/dental_and_oral_examination.json\nLoading module modules/dentures.json\nLoading module modules/dermatitis.json\nLoading module modules/dialysis.json\nLoading module modules/ear_infections.json\nLoading module modules/epilepsy.json\nLoading module modules/female_reproduction.json\nLoading module modules/fibromyalgia.json\nLoading module modules/food_allergies.json\nLoading module modules/gallstones.json\nLoading module modules/gout.json\nLoading module modules/hiv_care.json\nLoading Lookup Table: hiv_care.csv\nLoading module modules/hiv_diagnosis.json\nLoading Lookup Table: hiv_mortality_later.csv\nLoading Lookup Table: hiv_mortality_early.csv\nLoading Lookup Table: hiv_mortality_very_early.csv\nLoading Lookup Table: hiv_diagnosis_early.csv\nLoading Lookup Table: hiv_diagnosis_later.csv\nLoading module modules/home_health_treatment.json\nLoading module modules/home_hospice_snf.json\nLoading module modules/homelessness.json\nLoading module modules/hospice_treatment.json\nLoading module modules/hypertension.json\nLoading module modules/hypothyroidism.json\nLoading module modules/injuries.json\nLoading module modules/kidney_transplant.json\nLoading module modules/lung_cancer.json\nLoading module modules/lupus.json\nLoading module modules/mTBI.json\nLoading module modules/med_rec.json\nLoading module modules/mend_program.json\nLoading module modules/metabolic_syndrome_care.json\nLoading module modules/metabolic_syndrome_disease.json\nLoading module modules/myocardial_infarction.json\nLoading module modules/opioid_addiction.json\nLoading module modules/osteoarthritis.json\nLoading module modules/osteoporosis.json\nLoading module modules/pregnancy.json\nLoading module modules/prescribing_opioids_for_chronic_pain_and_treatment_of_oud.json\nLoading module modules/rheumatoid_arthritis.json\nLoading module modules/self_harm.json\nLoading module modules/sepsis.json\nLoading module modules/sexual_activity.json\nLoading module modules/sinusitis.json\nLoading module modules/sleep_apnea.json\nLoading module modules/sore_throat.json\nLoading module modules/spina_bifida.json\nLoading module modules/stable_ischemic_heart_disease.json\nLoading module modules/stroke.json\nLoading module modules/total_joint_replacement.json\nLoading module modules/trigger_bone_marrow_transplant.json\nLoading module modules/urinary_tract_infections.json\nLoading Lookup Table: uti.csv\nLoading Lookup Table: uti_recurrence.csv\nLoading module modules/veteran.json\nLoading module modules/veteran_hyperlipidemia.json\nLoading module modules/veteran_lung_cancer.json\nLoading module modules/veteran_mdd.json\nLoading module modules/veteran_prostate_cancer.json\nLoading module modules/veteran_ptsd.json\nLoading module modules/veteran_self_harm.json\nLoading module modules/veteran_substance_abuse_conditions.json\nLoading module modules/veteran_substance_abuse_treatment.json\nLoading module modules/vhd_aortic.json\nLoading Lookup Table: vhd_as.csv\nLoading Lookup Table: vhd_ar.csv\nLoading module modules/vhd_mitral.json\nLoading Lookup Table: vhd_mr.csv\nLoading Lookup Table: vhd_ms.csv\nLoading module modules/vhd_pulmonic.json\nLoading Lookup Table: vhd_ps.csv\nLoading Lookup Table: vhd_pr.csv\nLoading module modules/vhd_tricuspid.json\nLoading Lookup Table: vhd_tr.csv\nLoading Lookup Table: vhd_ts.csv\nLoading module modules/wellness_encounters.json\nRunning with options:\nPopulation: 10\nSeed: 12345\nProvider Seed:1746184779410\nReference Time: 1746184779410\nLocation: Massachusetts\nMin Age: 30\nMax Age: 85\n2 -- Rasheeda241 Mirian768 Lubowitz58 (35 y/o F) Boston, Massachusetts  (51042)\n1 -- Lorenzo669 Urrutia540 (51 y/o M) Chicopee, Massachusetts DECEASED (74857)\n3 -- Jacque955 Jin479 Bailey598 (58 y/o F) Shrewsbury, Massachusetts DECEASED (89377)\n1 -- Benjam\u00edn452 Apodaca347 (67 y/o M) Chicopee, Massachusetts  (100503)\n4 -- Keven605 Daugherty69 (50 y/o M) Dalton, Massachusetts  (72647)\n3 -- Telma919 Janee223 Prohaska837 (68 y/o F) Shrewsbury, Massachusetts  (101116)\n6 -- Darrell400 Muller251 (50 y/o M) Lowell, Massachusetts  (69071)\n5 -- Angela104 Funk324 (81 y/o F) South Yarmouth, Massachusetts  (118831)\n7 -- Claudia969 Escobedo608 (35 y/o F) Lynn, Massachusetts  (50954)\n9 -- Alfred550 West559 (31 y/o M) Belmont, Massachusetts  (46131)\n8 -- Luther918 MacGyver246 (81 y/o M) Boston, Massachusetts  (118717)\n10 -- Frederick289 Breitenberg711 (54 y/o M) Whitman, Massachusetts DECEASED (77538)\n10 -- Rich940 Hintz995 (31 y/o M) Whitman, Massachusetts DECEASED (43740)\n10 -- Van763 Ryan260 (10 y/o M) Whitman, Massachusetts DECEASED (13829)\n\n\ud83c\udf89 Success! Generated 18 CSV files:\n- conditions.csv\n- providers.csv\n- organizations.csv\n- allergies.csv\n- procedures.csv\n\nTotal records across all CSV files: 10 patients\n"}], "source": "# @title Synthea Patient Generator (CSV Version)\nimport os\nfrom IPython.display import clear_output\n\n# Configuration\nnum_patients = 10  # @param {type:\"integer\"}\nstate = \"Massachusetts\"  # @param [\"Massachusetts\", \"California\", \"New York\", \"Texas\", \"Florida\"]\nage_range = \"30-85\"  # @param {type:\"string\"}\nseed = 12345  # @param {type:\"integer\"}\n\n# Install Java\n!sudo apt-get update\n!sudo apt-get install -y openjdk-11-jdk-headless\nclear_output()\nprint(\"\u2705 Java installed\")\n\n# Download Synthea\n!wget -q https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\nclear_output()\nprint(\"\u2705 Synthea downloaded\")\n\n# Generate patients (using proper string substitution)\n!java -jar synthea-with-dependencies.jar \\\n  -p {num_patients} \\\n  -s {seed} \\\n  -a \"{age_range}\" \\\n  --exporter.baseDirectory \"./output\" \\\n  --exporter.fhir.export=False \\\n  --exporter.csv.export=True \\\n  {state}\n\n# Verify output\ncsv_output_path = \"./output/csv\"\nif os.path.exists(csv_output_path):\n    csv_files = [f for f in os.listdir(csv_output_path) if f.endswith('.csv')]\n    if csv_files:\n        print(f\"\\n\ud83c\udf89 Success! Generated {len(csv_files)} CSV files:\")\n        for file in csv_files[:5]:  # Show first 5 files\n            print(f\"- {file}\")\n        print(f\"\\nTotal records across all CSV files: {num_patients} patients\")\n    else:\n        print(\"\\n\u26a0 CSV directory exists but contains no CSV files\")\nelse:\n    print(\"\\n\u274c Generation failed. Common fixes:\")\n    print(\"1. Try reducing patient count (start with 10)\")\n    print(\"2. Check Java version:\")\n    !java -version\n    print(\"3. Disk space:\")\n    !df -h"}, {"cell_type": "code", "execution_count": 12, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "put: `/synthea_output/allergies.csv': File exists\nput: `/synthea_output/careplans.csv': File exists\nput: `/synthea_output/claims.csv': File exists\nput: `/synthea_output/claims_transactions.csv': File exists\nput: `/synthea_output/conditions.csv': File exists\nput: `/synthea_output/devices.csv': File exists\nput: `/synthea_output/encounters.csv': File exists\nput: `/synthea_output/imaging_studies.csv': File exists\nput: `/synthea_output/immunizations.csv': File exists\nput: `/synthea_output/medications.csv': File exists\nput: `/synthea_output/observations.csv': File exists\nput: `/synthea_output/organizations.csv': File exists\nput: `/synthea_output/patients.csv': File exists\nput: `/synthea_output/payer_transitions.csv': File exists\nput: `/synthea_output/payers.csv': File exists\nput: `/synthea_output/procedures.csv': File exists\nput: `/synthea_output/providers.csv': File exists\nput: `/synthea_output/supplies.csv': File exists\n"}], "source": "!hdfs dfs -mkdir -p /synthea_output\n!hdfs dfs -put ./output/csv/*.csv /synthea_output"}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "5iRZ0ROLV0j_", "tags": []}, "outputs": [], "source": "path = '/synthea_output/'"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) Creating the spark dataframes "}, {"cell_type": "code", "execution_count": 38, "metadata": {"id": "7LoWuO7AUH9a", "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#patient\nobservations = spark.read.csv(path+\"observations.csv\", header=True)\npatient = spark.read.csv(path+\"patients.csv\", header=True) \n\n#medical\ncareplans = spark.read.csv(path+\"careplans.csv\", header=True)\nconditions = spark.read.csv(path+\"conditions.csv\", header=True)\nprocedures=spark.read.csv(path+\"procedures.csv\", header=True)\nencounters = spark.read.csv(path+\"encounters.csv\", header=True)\nmedications = spark.read.csv(path+\"medications.csv\", header=True)\n\n#insurance and hospital\npayer_transitions=spark.read.csv(path+\"payer_transitions.csv\", header=True)\npayers=spark.read.csv(path+\"payers.csv\", header=True)\nproviders=spark.read.csv(path+\"providers.csv\", header=True)\norganizations=spark.read.csv(path+\"organizations.csv\", header=True)"}, {"cell_type": "code", "execution_count": 7, "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "['Id',\n 'START',\n 'STOP',\n 'PATIENT',\n 'ORGANIZATION',\n 'PROVIDER',\n 'PAYER',\n 'ENCOUNTERCLASS',\n 'CODE',\n 'DESCRIPTION',\n 'BASE_ENCOUNTER_COST',\n 'TOTAL_CLAIM_COST',\n 'PAYER_COVERAGE',\n 'REASONCODE',\n 'REASONDESCRIPTION']"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "encounters.columns"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3) Cleaning dataframes and renaming variables "}, {"cell_type": "code", "execution_count": 39, "metadata": {"id": "AlZ6m6w-UVRC", "tags": []}, "outputs": [], "source": "# renaming columns\n\npatient = (\n    patient.withColumnRenamed(\"Id\", \"patient_id\")\n           .withColumnRenamed(\"MARITAL\", \"patient_marital\")\n           .withColumnRenamed(\"RACE\", \"patient_race\")\n           .withColumnRenamed(\"ETHNICITY\", \"patient_ethnicity\")\n           .withColumnRenamed(\"GENDER\", \"patient_gender\")\n           .withColumnRenamed(\"ZIP\", \"patient_zip\")\n)\n\nencounters = (\n    encounters.withColumnRenamed(\"PATIENT\", \"patient_id\")\n              .withColumnRenamed(\"Id\", \"encounter_id\")\n              .withColumnRenamed(\"DESCRIPTION\", \"encounter_description\")\n              .withColumnRenamed(\"CODE\", \"encounter_code\")\n              .withColumnRenamed(\"START\", \"encounter_start\")\n              .withColumn(\"encounter_start\", to_date(\"encounter_start\"))\n              .withColumnRenamed(\"STOP\", \"encounter_stop\")\n              .withColumn(\"encounter_stop\", to_date(\"encounter_stop\"))\n              .withColumn(\"PATIENT COST\", col(\"TOTAL_CLAIM_COST\") - col(\"PAYER_COVERAGE\"))\n              .withColumnRenamed(\"PAYER\", \"payer_id\")\n              .withColumnRenamed(\"ORGANIZATION\", \"organization_id\")\n              .withColumnRenamed(\"PROVIDER\", \"provider_id\")\n)\n\ncareplans = (\n    careplans.withColumnRenamed(\"PATIENT\", \"patient_id\")\n             .withColumnRenamed(\"Id\", \"careplan_id\")\n             .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n             .withColumnRenamed(\"DESCRIPTION\", \"careplan_descriptions\")\n             .withColumnRenamed(\"CODE\", \"careplan_code\")\n)\n\nprocedures = (\n    procedures.withColumnRenamed(\"PATIENT\", \"patient_id\")\n              .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n              .withColumnRenamed(\"DESCRIPTION\", \"procedure_descriptions\")\n              .withColumnRenamed(\"CODE\", \"procedure_code\")\n              .withColumnRenamed(\"DATE\", \"procedure_date\")\n              .withColumnRenamed(\"BASE_COST\", \"procedure_cost\")\n)\n\nconditions = (\n    conditions.withColumnRenamed(\"PATIENT\", \"patient_id\")\n              .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n              .withColumnRenamed(\"DESCRIPTION\", \"condition_description\")\n              .withColumnRenamed(\"CODE\", \"condition_code\")\n              .withColumnRenamed(\"START\", \"condition_start\")\n              .withColumnRenamed(\"END\", \"condition_end\")\n)\n\nobservations = (\n    observations.withColumnRenamed(\"PATIENT\", \"patient_id\")\n                .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n                .withColumnRenamed(\"DATE\", \"observation_date\")\n                .withColumn(\"observation_date\", to_date(\"observation_date\"))\n)\n\nmedications = (\n    medications.withColumnRenamed(\"START\", \"medication_start\")\n               .withColumn(\"medication_start\", to_date(\"medication_start\"))\n               .withColumnRenamed(\"STOP\", \"medication_stop\")\n               .withColumn(\"medication_stop\", to_date(\"medication_stop\"))\n               .withColumnRenamed(\"PATIENT\", \"patient_id\")\n               .withColumnRenamed(\"PAYER\", \"payer_id\")\n               .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n               .withColumnRenamed(\"CODE\", \"medication_code\")\n               .withColumnRenamed(\"DESCRIPTION\", \"medication_description\")\n)\n\npayer_transitions = (\n    payer_transitions.withColumnRenamed(\"PATIENT\", \"patient_id\")\n                     .withColumnRenamed(\"PAYER\", \"payer_id\")\n)\n\npayers = (\n    payers.withColumnRenamed(\"Id\", \"payer_id\")\n          .withColumnRenamed(\"NAME\", \"payer_name\")\n          .withColumnRenamed(\"OWNERSHIP\", \"payer_ownership\")\n)\n\nproviders = (\n    providers.withColumnRenamed(\"Id\", \"provider_id\")\n             .withColumnRenamed(\"SPECIALITY\", \"provider_specialty\")\n)\n\norganizations = (\n    organizations.withColumnRenamed(\"Id\", \"organization_id\")\n                 .withColumnRenamed(\"NAME\", \"organization_name\")\n                 .withColumnRenamed(\"ZIP\", \"organization_zip\")\n                 .withColumn(\"organization_zip\", substring(col(\"organization_zip\").cast(\"string\"), 1, 5))\n)"}, {"cell_type": "code", "execution_count": 40, "metadata": {"tags": []}, "outputs": [], "source": "#can do this too \nencounters = (\n    encounters\n    .join(payers.select(\"payer_id\", \"payer_name\", \"payer_ownership\"), on=\"payer_id\", how=\"left\")\n    .join(organizations.select(\"organization_id\", \"organization_name\", \"organization_zip\"), on=\"organization_id\", how=\"left\")\n    .join(providers.select(\"provider_id\", \"provider_specialty\"), on=\"provider_id\", how=\"left\")\n    .join(procedures.select(\"encounter_id\", \"procedure_descriptions\", \"procedure_code\"), on=\"encounter_id\", how=\"left\")\n    .join(patient.select(\"patient_id\", \"BIRTHDATE\", \"patient_marital\", \"patient_race\", \"patient_ethnicity\", \"patient_gender\", \"patient_zip\"), on=\"patient_id\", how=\"left\")\n    .withColumn(\"age_at_encounter\", floor(datediff(col(\"encounter_start\"), col(\"BIRTHDATE\")) / 365.25))\n)"}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "2mNaI6flhLXz", "tags": []}, "outputs": [{"data": {"text/plain": "['patient_id',\n 'encounter_id',\n 'provider_id',\n 'organization_id',\n 'payer_id',\n 'encounter_start',\n 'encounter_stop',\n 'ENCOUNTERCLASS',\n 'encounter_code',\n 'encounter_description',\n 'BASE_ENCOUNTER_COST',\n 'TOTAL_CLAIM_COST',\n 'PAYER_COVERAGE',\n 'REASONCODE',\n 'REASONDESCRIPTION',\n 'PATIENT COST',\n 'payer_name',\n 'payer_ownership',\n 'organization_name',\n 'organization_zip',\n 'provider_specialty',\n 'procedure_descriptions',\n 'procedure_code',\n 'BIRTHDATE',\n 'patient_marital',\n 'patient_race',\n 'patient_ethnicity',\n 'patient_gender',\n 'patient_zip',\n 'age_at_encounter']"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "encounters.columns"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Split by topic using LDA and train regession model per topic "}, {"cell_type": "markdown", "metadata": {}, "source": "### LDA using encounter description"}, {"cell_type": "code", "execution_count": 41, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.sql.functions import col, lower, regexp_replace\n"}, {"cell_type": "code", "execution_count": 42, "metadata": {"tags": []}, "outputs": [], "source": "#clean the text first \nencounters = encounters.filter(col(\"encounter_description\").isNotNull())\nencounters = encounters.withColumn(\n    \"clean_text\",\n    lower(regexp_replace(col(\"encounter_description\"), \"[^a-zA-Z\\\\s]\", \"\"))\n)"}, {"cell_type": "code", "execution_count": 43, "metadata": {"tags": []}, "outputs": [], "source": "# Additional Stopwords \n#ARE WE REMOVING TOO MANY STOPWORDS \nstopwords=StopWordsRemover.loadDefaultStopWords(\"english\") + [\n    \"patient\", \"doctor\", \"visit\", \"care\", \"provider\", \"encounter\", \"hospital\",\"room\",\"admission\",\"procedure\"\n]\n\n# Text Preprocessing Pipeline\ntokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords=stopwords)\nvectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")"}, {"cell_type": "code", "execution_count": 44, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Fit and Transform Data\ntokenized_df = tokenizer.transform(encounters)\nfiltered_df = remover.transform(tokenized_df)\nvectorizer_model = vectorizer.fit(filtered_df)\nvectorized_df = vectorizer_model.transform(filtered_df)"}, {"cell_type": "code", "execution_count": 45, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Train LDA Model\n#NEED TO INCREASE K WHEN INCREASING PATIENTS \nlda = LDA(k=2, maxIter=10, featuresCol=\"features\")\nlda_model = lda.fit(vectorized_df)"}, {"cell_type": "code", "execution_count": 46, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+---------------------------------------------------------------------------------------------------------+\n|topic|termIndices    |termWeights                                                                                              |\n+-----+---------------+---------------------------------------------------------------------------------------------------------+\n|0    |[1, 0, 2, 5, 6]|[0.3204907381968033, 0.30975259100756763, 0.05537340064752298, 0.04370772966588497, 0.029847597655479296]|\n|1    |[2, 0, 1, 3, 4]|[0.438065940196969, 0.08362018771314891, 0.07016182811901082, 0.05882397489558225, 0.04377200215132048]  |\n+-----+---------------+---------------------------------------------------------------------------------------------------------+\n\n"}], "source": "# Show Topics\ntopics = lda_model.describeTopics(5)\ntopics.show(truncate=False)"}, {"cell_type": "code", "execution_count": 47, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+------------------------------------------------------+\n|topic|top_words                                             |\n+-----+------------------------------------------------------+\n|0    |[examination, general, check, problem, hospice]       |\n|1    |[check, general, examination, regimetherapy, prenatal]|\n+-----+------------------------------------------------------+\n\n"}], "source": "vocab = vectorizer_model.vocabulary\ntopics_with_words = topics.rdd.map(\n    lambda row: (row['topic'], [vocab[i] for i in row['termIndices']])\n).toDF([\"topic\", \"top_words\"])\n\ntopics_with_words.show(truncate=False)"}, {"cell_type": "code", "execution_count": 48, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\nimport numpy as np\n\nencounters = lda_model.transform(vectorized_df)\n\nget_topic = udf(lambda v: int(np.argmax(v)), IntegerType())\nencounters = encounters.withColumn(\"topic\", get_topic(col(\"topicDistribution\")))"}, {"cell_type": "code", "execution_count": 49, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType\n\nnumeric_columns = [\"BASE_ENCOUNTER_COST\", \"TOTAL_CLAIM_COST\", \"PAYER_COVERAGE\", \"age_at_encounter\"]\n\nfor column in numeric_columns:\n    encounters = encounters.withColumn(column, col(column).cast(DoubleType()))\n"}, {"cell_type": "code", "execution_count": 50, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import StringIndexer\n\ncategorical_columns = [\n    \"patient_gender\", \"patient_race\", \"patient_ethnicity\",\n    \"patient_marital\", \"provider_specialty\", \"organization_zip\",\"patient_zip\",'REASONDESCRIPTION', \n 'payer_ownership',\n 'organization_name',\n]\n\nfor cat_col in categorical_columns:\n    indexer = StringIndexer(inputCol=cat_col, outputCol=cat_col + \"_index\", handleInvalid=\"keep\")\n    encounters = indexer.fit(encounters).transform(encounters)\n"}, {"cell_type": "code", "execution_count": 51, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml.feature import VectorAssembler\n\n# Define final feature list\nfinal_features = numeric_columns + [col + \"_index\" for col in categorical_columns] + [\"topic\"]\n\n# Drop rows with nulls in important columns\nencounters = encounters.na.drop(subset=final_features + [\"PATIENT COST\"])\n\n# Assemble into feature vector\nassembler = VectorAssembler(inputCols=final_features, outputCol=\"final_features\")\nencounters = assembler.transform(encounters)\n"}, {"cell_type": "code", "execution_count": 52, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.regression import LinearRegression\n\nmodels = {}\ntopics = encounters.select(\"topic\").distinct().rdd.flatMap(lambda x: x).collect()\n\nfor t in topics:\n    topic_df = encounters.filter(col(\"topic\") == t)\n    lr = LinearRegression(featuresCol=\"final_features\", labelCol=\"PATIENT COST\",regParam=0.1) #might not need regParam with a bigger dataset \n    model = lr.fit(topic_df)\n    models[t] = model\n"}, {"cell_type": "code", "execution_count": 53, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------------------------+------------------+------------------+-----+\n|patient_id                          |PATIENT COST      |prediction        |topic|\n+------------------------------------+------------------+------------------+-----+\n|4113255f-4e35-506a-ddef-4429caa17ffc|85.55             |87.16762483511698 |1    |\n|4113255f-4e35-506a-ddef-4429caa17ffc|180.79999999999995|181.01157733737256|1    |\n|4113255f-4e35-506a-ddef-4429caa17ffc|142.58            |140.90154432465138|1    |\n|4113255f-4e35-506a-ddef-4429caa17ffc|948.35            |946.9532141920059 |1    |\n|92675303-ca5b-136a-169b-e764c5753f06|871.06            |872.57503113922   |1    |\n|92675303-ca5b-136a-169b-e764c5753f06|808.91            |812.0693849090785 |1    |\n|4113255f-4e35-506a-ddef-4429caa17ffc|142.58            |140.92146436156523|1    |\n|4113255f-4e35-506a-ddef-4429caa17ffc|85.55             |85.33199619772024 |1    |\n|92675303-ca5b-136a-169b-e764c5753f06|3536.75           |3536.8345181877817|1    |\n|92675303-ca5b-136a-169b-e764c5753f06|3536.75           |3536.8345181877817|1    |\n+------------------------------------+------------------+------------------+-----+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from functools import reduce\nfrom pyspark.sql import DataFrame\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\npredictions_list = []\n\nfor t, model in models.items():\n    topic_df = encounters.filter(col(\"topic\") == t)\n    preds = model.transform(topic_df)\n    predictions_list.append(preds)\n\nfinal_predictions = unionAll(*predictions_list)\nfinal_predictions.select(\"patient_id\",\"PATIENT COST\", \"prediction\", \"topic\").show(10, truncate=False)\n"}, {"cell_type": "code", "execution_count": 54, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 596:============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 RMSE: 1.72\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(\n    labelCol=\"PATIENT COST\",\n    predictionCol=\"prediction\",\n    metricName=\"rmse\"  \n)\n\nrmse = evaluator.evaluate(final_predictions)\nprint(f\"\u2705 RMSE: {rmse:.2f}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### LDA using encounter description + demographic information\n#### actually maybe i shouldn't merge them into 1 text, it doesnt work"}, {"cell_type": "code", "execution_count": 56, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import concat_ws, col, lower, regexp_replace\n\n# Create demographics text\nencounters = encounters.withColumn(\n    \"demographic_text\",\n    concat_ws(\" \", \"patient_gender\", \"patient_race\", \"patient_ethnicity\", \"patient_marital\",\"age_at_encounter\")\n)\n\n# Combine with encounter_description\nencounters = encounters.withColumn(\n    \"full_text\",\n    concat_ws(\" \", \"encounter_description\", \"demographic_text\")\n)\n\n# Clean it\nencounters = encounters.withColumn(\n    \"clean_text_demo\",\n    lower(regexp_replace(col(\"full_text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n)\n"}, {"cell_type": "code", "execution_count": 57, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\nfrom pyspark.ml.clustering import LDA\n\n# Tokenize\ntokenizer = Tokenizer(inputCol=\"clean_text_demo\", outputCol=\"words_demo\")\nwords_df = tokenizer.transform(encounters)\n\n# Remove stopwords\nstopwords = StopWordsRemover.loadDefaultStopWords(\"english\") + [\n    \"patient\", \"doctor\", \"visit\", \"care\", \"provider\", \"hospital\", \"room\", \"procedure\"\n]\nremover = StopWordsRemover(inputCol=\"words_demo\", outputCol=\"filtered_demo\", stopWords=stopwords)\nfiltered_df = remover.transform(words_df)\n\n# Vectorize\nvectorizer_demo = CountVectorizer(inputCol=\"filtered_demo\", outputCol=\"features_demo\")\nvector_model_demo = vectorizer_demo.fit(filtered_df)\nvectorized_df = vector_model_demo.transform(filtered_df)\n\n# Train LDA\nlda_demo = LDA(k=5, maxIter=10, featuresCol=\"features_demo\",topicDistributionCol=\"topicDistribution_demo\")\nlda_model_demo = lda_demo.fit(vectorized_df)\n\n# Add topicDistribution and topic\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\nimport numpy as np\n\nvectorized_df = lda_model_demo.transform(vectorized_df)\nget_topic = udf(lambda v: int(np.argmax(v)), IntegerType())\nvectorized_df = vectorized_df.withColumn(\"topic_demo\", get_topic(col(\"topicDistribution_demo\")))\n"}, {"cell_type": "code", "execution_count": 61, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----------------------------------------------------+\n|topic|top_words                                           |\n+-----+----------------------------------------------------+\n|0    |[unit, environment, check, department, death]       |\n|1    |[m, white, general, examination, nonhispanic]       |\n|2    |[black, f, nonhispanic, m, regimetherapy]           |\n|3    |[m, white, encounter, check, nonhispanic]           |\n|4    |[active, administration, immunity, vaccine, produce]|\n+-----+----------------------------------------------------+\n\n"}], "source": "# Show Topics\ntopics_demo = lda_model_demo.describeTopics(5)\nvocab_demo = vector_model_demo.vocabulary\ntopics_with_words_demo = topics_demo.rdd.map(\n    lambda row: (row['topic'], [vocab_demo[i] for i in row['termIndices']])\n).toDF([\"topic\", \"top_words\"])\n\ntopics_with_words_demo.show(truncate=False)"}, {"cell_type": "code", "execution_count": 62, "metadata": {"tags": []}, "outputs": [], "source": "final_features_demo = numeric_columns + [col + \"_index\" for col in categorical_columns] + [\"topic_demo\"]\n\nencounters_demo = vectorized_df.na.drop(subset=final_features_demo + [\"PATIENT COST\"])\n\n# Assemble feature vector\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler_demo = VectorAssembler(inputCols=final_features_demo, outputCol=\"final_features_demo\")\nencounters_demo = assembler_demo.transform(encounters_demo)"}, {"cell_type": "code", "execution_count": 63, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.regression import LinearRegression\n\nmodels_demo = {}\ntopics_demo = encounters_demo.select(\"topic_demo\").distinct().rdd.flatMap(lambda x: x).collect()\n\nfor t in topics_demo:\n    topic_df = encounters_demo.filter(col(\"topic_demo\") == t)\n    lr = LinearRegression(featuresCol=\"final_features_demo\", labelCol=\"PATIENT COST\", regParam=0.1)\n    model = lr.fit(topic_df)\n    models_demo[t] = model\n"}, {"cell_type": "code", "execution_count": 36, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 401:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------------------------+------------+------------------+----------+\n|patient_id                          |PATIENT COST|prediction        |topic_demo|\n+------------------------------------+------------+------------------+----------+\n|92675303-ca5b-136a-169b-e764c5753f06|988.17      |987.1867254089184 |1         |\n|4113255f-4e35-506a-ddef-4429caa17ffc|0.0         |8.011062295192914 |1         |\n|4113255f-4e35-506a-ddef-4429caa17ffc|237.36      |243.266052491055  |1         |\n|4113255f-4e35-506a-ddef-4429caa17ffc|919.9       |917.0025606007193 |1         |\n|92675303-ca5b-136a-169b-e764c5753f06|917.0       |917.0193329347114 |1         |\n|92675303-ca5b-136a-169b-e764c5753f06|871.06      |873.2203742749697 |1         |\n|92675303-ca5b-136a-169b-e764c5753f06|1161.11     |1159.3679832971934|1         |\n|92675303-ca5b-136a-169b-e764c5753f06|921.58      |922.7738067665181 |1         |\n|92675303-ca5b-136a-169b-e764c5753f06|1057.58     |1057.444488004062 |1         |\n|92675303-ca5b-136a-169b-e764c5753f06|934.74      |935.9343340348709 |1         |\n+------------------------------------+------------+------------------+----------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from functools import reduce\nfrom pyspark.sql import DataFrame\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\npredictions_demo_list = []\n\nfor t, model in models_demo.items():\n    topic_df = encounters_demo.filter(col(\"topic_demo\") == t)\n    preds = model.transform(topic_df)\n    predictions_demo_list.append(preds)\n\nfinal_predictions_demo = unionAll(*predictions_demo_list)\nfinal_predictions_demo.select(\"patient_id\",\"PATIENT COST\", \"prediction\", \"topic_demo\").show(10, truncate=False)\n"}, {"cell_type": "code", "execution_count": 37, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 407:==========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 RMSE_demo: 2.17\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(\n    labelCol=\"PATIENT COST\",\n    predictionCol=\"prediction\",\n    metricName=\"rmse\"  \n)\n\nrmse_demo = evaluator.evaluate(final_predictions_demo)\nprint(f\"\u2705 RMSE_demo: {rmse_demo:.2f}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### LDA using encounter description + demographic information + procedure information"}, {"cell_type": "code", "execution_count": 68, "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import concat_ws, col, lower, regexp_replace\n\n# Combine with procedure_description with encounter_description\nencounters = encounters.withColumn(\n    \"enc_proc\",\n    concat_ws(\" \", \"encounter_description\", \"procedure_descriptions\")\n)\n\n# Clean it\nencounters = encounters.withColumn(\n    \"clean_encproc\",\n    lower(regexp_replace(col(\"enc_proc\"), \"[^a-zA-Z\\\\s]\", \"\"))\n)\n"}, {"cell_type": "code", "execution_count": 70, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\nfrom pyspark.ml.clustering import LDA\n\n# Tokenize\ntokenizer = Tokenizer(inputCol=\"clean_encproc\", outputCol=\"words_encproc\")\nwords_df = tokenizer.transform(encounters)\n\n# Remove stopwords\nstopwords = StopWordsRemover.loadDefaultStopWords(\"english\") + [\n    \"patient\", \"doctor\", \"visit\", \"care\", \"provider\", \"hospital\", \"room\", \"procedure\"\n]\nremover = StopWordsRemover(inputCol=\"words_encproc\", outputCol=\"filtered_encproc\", stopWords=stopwords)\nfiltered_df = remover.transform(words_df)\n\n# Vectorize\nvectorizer_encproc = CountVectorizer(inputCol=\"filtered_encproc\", outputCol=\"features_encproc\")\nvector_model_encproc = vectorizer_encproc.fit(filtered_df)\nvectorized_df = vector_model_encproc.transform(filtered_df)\n\n# Train LDA\nlda_encproc = LDA(k=5, maxIter=10, featuresCol=\"features_encproc\",topicDistributionCol=\"topicDistribution_encproc\")\nlda_model_encproc = lda_encproc.fit(vectorized_df)\n\n# Add topicDistribution and topic\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\nimport numpy as np\n\nvectorized_df = lda_model_encproc.transform(vectorized_df)\nget_topic = udf(lambda v: int(np.argmax(v)), IntegerType())\nvectorized_df = vectorized_df.withColumn(\"topic_encproc\", get_topic(col(\"topicDistribution_encproc\")))"}, {"cell_type": "code", "execution_count": 71, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------------------------------------------------------+\n|topic|top_words                                                    |\n+-----+-------------------------------------------------------------+\n|0    |[contraceptive, subcutaneous, insertion, bilateral, ligation]|\n|1    |[administration, produce, regimetherapy, prenatal, immunity] |\n|2    |[encounter, check, dental, using, removal]                   |\n|3    |[general, examination, assessment, health, needs]            |\n|4    |[screening, examination, general, depression, admission]     |\n+-----+-------------------------------------------------------------+\n\n"}], "source": "# Show Topics\ntopics_encproc = lda_model_encproc.describeTopics(5)\nvocab_encproc = vector_model_encproc.vocabulary\ntopics_with_words_encproc = topics_encproc.rdd.map(\n    lambda row: (row['topic'], [vocab_encproc[i] for i in row['termIndices']])\n).toDF([\"topic\", \"top_words\"])\n\ntopics_with_words_encproc.show(truncate=False)"}, {"cell_type": "code", "execution_count": 72, "metadata": {"tags": []}, "outputs": [], "source": "final_features_encproc = numeric_columns + [col + \"_index\" for col in categorical_columns] + [\"topic_encproc\"]\n\nencounters_encproc = vectorized_df.na.drop(subset=final_features_encproc + [\"PATIENT COST\"])\n\n# Assemble feature vector\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler_encproc = VectorAssembler(inputCols=final_features_encproc, outputCol=\"final_features_encproc\")\nencounters_encproc = assembler_encproc.transform(encounters_encproc)"}, {"cell_type": "code", "execution_count": 73, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.regression import LinearRegression\n\nmodels_encproc = {}\ntopics_encproc = encounters_encproc.select(\"topic_encproc\").distinct().rdd.flatMap(lambda x: x).collect()\n\nfor t in topics_encproc:\n    topic_df = encounters_encproc.filter(col(\"topic_encproc\") == t)\n    lr = LinearRegression(featuresCol=\"final_features_encproc\", labelCol=\"PATIENT COST\", regParam=0.1)\n    model = lr.fit(topic_df)\n    models_encproc[t] = model"}, {"cell_type": "code", "execution_count": 75, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 904:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------------------------+------------+------------------+-------------+\n|patient_id                          |PATIENT COST|prediction        |topic_encproc|\n+------------------------------------+------------+------------------+-------------+\n|4113255f-4e35-506a-ddef-4429caa17ffc|278.58      |278.55545603768104|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|224.48      |224.51680304515932|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n|4113255f-4e35-506a-ddef-4429caa17ffc|19615.09    |19614.387558020153|1            |\n+------------------------------------+------------+------------------+-------------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from functools import reduce\nfrom pyspark.sql import DataFrame\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\npredictions_encproc_list = []\n\nfor t, model in models_encproc.items():\n    topic_df = encounters_encproc.filter(col(\"topic_encproc\") == t)\n    preds = model.transform(topic_df)\n    predictions_encproc_list.append(preds)\n\nfinal_predictions_encproc = unionAll(*predictions_encproc_list)\nfinal_predictions_encproc.select(\"patient_id\",\"PATIENT COST\", \"prediction\", \"topic_encproc\").show(10, truncate=False)\n"}, {"cell_type": "code", "execution_count": 76, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 910:=============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 RMSE_encproc: 1.92\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(\n    labelCol=\"PATIENT COST\",\n    predictionCol=\"prediction\",\n    metricName=\"rmse\"  \n)\n\nrmse_encproc = evaluator.evaluate(final_predictions_encproc)\nprint(f\"\u2705 RMSE_encproc: {rmse_encproc:.2f}\")\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 4}