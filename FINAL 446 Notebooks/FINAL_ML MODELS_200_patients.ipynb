{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3mqRk7RpXDd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sql_f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date, datediff, floor, col, avg, substring, when, length, lpad\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to measure execution time...\n",
    "def time_execution(task_name, func):\n",
    "    start_time = time.time()\n",
    "    result = func()\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"{task_name} executed in {duration:.2f} seconds\")\n",
    "    return result, duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthea Batch Generator with Concatenation of csv files\n",
    "\n",
    "#Synthea Configuration \n",
    "total_patients = 200  \n",
    "batch_size = 20\n",
    "state = \"Massachusetts\"  \n",
    "age_range = \"30-85\"  \n",
    "base_seed = 12345  \n",
    "output_dir = \"./output\" \n",
    "\n",
    "#Initial setup to ensure java is installed as well as the \"synthea-with-dependencies.jar\" file needed to create the patient records\n",
    "def setup_environment():\n",
    "    \"\"\"Install Java and download Synthea if not already present\"\"\"\n",
    "    if not os.path.exists(\"/usr/bin/java\"):\n",
    "        print(\"Installing Java...\")\n",
    "        !sudo apt-get update -qq > /dev/null\n",
    "        !sudo apt-get install -y openjdk-11-jdk-headless > /dev/null\n",
    "        clear_output()\n",
    "        print(\"Java installed\")\n",
    "    \n",
    "    if not os.path.exists(\"synthea-with-dependencies.jar\"):\n",
    "        print(\"Downloading Synthea...\")\n",
    "        !wget -q https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\n",
    "        clear_output()\n",
    "        print(\"Synthea downloaded\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Batch generation of patients to avoid errors due to kernel reconnecting...\n",
    "def generate_batch(batch_num, patients_in_batch, current_seed):\n",
    "    \"\"\"Generate one batch of synthetic patients\"\"\"\n",
    "    try:\n",
    "        print(f\"Batch {batch_num}: Generating {patients_in_batch} patients (seed: {current_seed})\")\n",
    "        \n",
    "        !java -jar synthea-with-dependencies.jar \\\n",
    "          -p {patients_in_batch} \\\n",
    "          -s {current_seed} \\\n",
    "          -a \"{age_range}\" \\\n",
    "          --exporter.baseDirectory \"{output_dir}\" \\\n",
    "          --exporter.fhir.export=False \\\n",
    "          --exporter.csv.export=True \\\n",
    "          --exporter.csv.folder_per_run=true \\\n",
    "          {state}\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {batch_num}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "#Combining csv files based on their filenames\n",
    "def concatenate_all_csvs():\n",
    "    \"\"\"Combine all generated CSV files by their type\"\"\"\n",
    "    csv_files = glob.glob(f\"{output_dir}/**/*.csv\", recursive=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found to concatenate\")\n",
    "        return None\n",
    "    \n",
    "    combined_data = {}\n",
    "    \n",
    "    for filepath in csv_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        file_type = filename.split('.')[0]\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            if file_type in combined_data:\n",
    "                combined_data[file_type] = pd.concat([combined_data[file_type], df], ignore_index=True)\n",
    "            else:\n",
    "                combined_data[file_type] = df\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {filename}: {str(e)}\")\n",
    "    \n",
    "    #Saving combined files\n",
    "    combined_dir = f\"{output_dir}/combined\"\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "    \n",
    "    for file_type, df in combined_data.items():\n",
    "        output_path = f\"{combined_dir}/{file_type}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved {len(df)} records to {output_path}\")\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "#Calling functions from above:\n",
    "if __name__ == \"__main__\":\n",
    "    setup_environment()\n",
    "\n",
    "    start_time = time.time()\n",
    "    completed = 0\n",
    "    batch_num = 1\n",
    "    \n",
    "    print(f\"Starting generation of {total_patients} patients in batches of {batch_size}...\")\n",
    "    \n",
    "    while completed < total_patients:\n",
    "        current_batch_size = min(batch_size, total_patients - completed)\n",
    "        current_seed = base_seed + completed\n",
    "        \n",
    "        #Retrying logic (3 attempts per batch)\n",
    "        success = False\n",
    "        for attempt in range(3):\n",
    "            if generate_batch(batch_num, current_batch_size, current_seed):\n",
    "                success = True\n",
    "                break\n",
    "            time.sleep(5)\n",
    "        \n",
    "        if success:\n",
    "            completed += current_batch_size\n",
    "            progress = completed / total_patients * 100\n",
    "            print(f\"Progress: {completed}/{total_patients} ({progress:.1f}%)\")\n",
    "            batch_num += 1\n",
    "        else:\n",
    "            print(f\"Failed batch {batch_num} after 3 attempts\")\n",
    "            break\n",
    "    \n",
    "    #Concatenate results\n",
    "    print(\"\\nCombining all CSV files...\")\n",
    "    combined_data = concatenate_all_csvs()\n",
    "\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"COMPLETED IN {elapsed:.1f} MINUTES\")\n",
    "    print(f\"Total patients generated: {completed}/{total_patients}\")\n",
    "    \n",
    "    if combined_data:\n",
    "        print(\"\\nCOMBINED FILES SUMMARY:\")\n",
    "        for file_type, df in combined_data.items():\n",
    "            print(f\"- {file_type}.csv: {len(df)} records\")\n",
    "    else:\n",
    "        print(\"\\nNo files were combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving combined csv files into HDFS\n",
    "!hdfs dfs -mkdir -p /synthea_output 2> /dev/null\n",
    "!hdfs dfs -put -f ./output/combined/*.csv /synthea_output/\n",
    "!rm -rf ./output 2> /dev/null && echo \"Local files cleaned up\" || echo \"Error cleaning local files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this cell to save files to local downloads:\n",
    "\n",
    "# !mkdir -p ~/Downloads/synthea_csvs\n",
    "# !hdfs dfs -get /synthea_output/*.csv ~/Downloads/synthea_csvs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating the spark dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iRZ0ROLV0j_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting path to HDFS folder\n",
    "path = '/synthea_output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LoWuO7AUH9a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Patient files\n",
    "observations = spark.read.csv(path+\"observations.csv\", header=True)\n",
    "patient = spark.read.csv(path+\"patients.csv\", header=True) \n",
    "#Medical files\n",
    "careplans = spark.read.csv(path+\"careplans.csv\", header=True)\n",
    "conditions = spark.read.csv(path+\"conditions.csv\", header=True)\n",
    "procedures=spark.read.csv(path+\"procedures.csv\", header=True)\n",
    "encounters = spark.read.csv(path+\"encounters.csv\", header=True)\n",
    "medications = spark.read.csv(path+\"medications.csv\", header=True)\n",
    "#Insurance and hospital files\n",
    "payer_transitions=spark.read.csv(path+\"payer_transitions.csv\", header=True)\n",
    "payers=spark.read.csv(path+\"payers.csv\", header=True)\n",
    "providers=spark.read.csv(path+\"providers.csv\", header=True)\n",
    "organizations=spark.read.csv(path+\"organizations.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cleaning dataframes and renaming variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlZ6m6w-UVRC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Renaming columns\n",
    "patient = (\n",
    "    patient.withColumnRenamed(\"Id\", \"patient_id\")\n",
    "           .withColumnRenamed(\"MARITAL\", \"patient_marital\")\n",
    "           .withColumnRenamed(\"RACE\", \"patient_race\")\n",
    "           .withColumnRenamed(\"ETHNICITY\", \"patient_ethnicity\")\n",
    "           .withColumnRenamed(\"GENDER\", \"patient_gender\")\n",
    "           .withColumnRenamed(\"ZIP\", \"patient_zip\")\n",
    ")\n",
    "encounters = (\n",
    "    encounters.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "              .withColumnRenamed(\"Id\", \"encounter_id\")\n",
    "              .withColumnRenamed(\"DESCRIPTION\", \"encounter_discription\")\n",
    "              .withColumnRenamed(\"CODE\", \"encounter_code\")\n",
    "              .withColumnRenamed(\"START\", \"encounter_start\")\n",
    "              .withColumn(\"encounter_start\", to_date(\"encounter_start\"))\n",
    "              .withColumnRenamed(\"STOP\", \"encounter_stop\")\n",
    "              .withColumn(\"encounter_stop\", to_date(\"encounter_stop\"))\n",
    "              .withColumn(\"PATIENT COST\", col(\"TOTAL_CLAIM_COST\") - col(\"PAYER_COVERAGE\"))\n",
    "              .withColumnRenamed(\"PAYER\", \"payer_id\")\n",
    "              .withColumnRenamed(\"ORGANIZATION\", \"organization_id\")\n",
    "              .withColumnRenamed(\"PROVIDER\", \"provider_id\")\n",
    ")\n",
    "careplans = (\n",
    "    careplans.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "             .withColumnRenamed(\"Id\", \"careplan_id\")\n",
    "             .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n",
    "             .withColumnRenamed(\"DESCRIPTION\", \"careplan_descriptions\")\n",
    "             .withColumnRenamed(\"CODE\", \"careplan_code\")\n",
    ")\n",
    "procedures = (\n",
    "    procedures.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "              .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n",
    "              .withColumnRenamed(\"DESCRIPTION\", \"procedure_descriptions\")\n",
    "              .withColumnRenamed(\"CODE\", \"procedure_code\")\n",
    "              .withColumnRenamed(\"DATE\", \"procedure_date\")\n",
    "              .withColumnRenamed(\"BASE_COST\", \"procedure_cost\")\n",
    ")\n",
    "conditions = (\n",
    "    conditions.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "              .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n",
    "              .withColumnRenamed(\"DESCRIPTION\", \"condition_description\")\n",
    "              .withColumnRenamed(\"CODE\", \"condition_code\")\n",
    "              .withColumnRenamed(\"START\", \"condition_start\")\n",
    "              .withColumnRenamed(\"END\", \"condition_end\")\n",
    ")\n",
    "observations = (\n",
    "    observations.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "                .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n",
    "                .withColumnRenamed(\"DATE\", \"observation_date\")\n",
    "                .withColumn(\"observation_date\", to_date(\"observation_date\"))\n",
    "                .withColumn(\"obs_value\", col(\"VALUE\").cast(\"double\"))\n",
    "                .withColumnRenamed(\"CODE\", \"observation_code\")\n",
    "                .withColumnRenamed(\"DESCRIPTION\", \"observation_description\")\n",
    ")\n",
    "medications = (\n",
    "    medications.withColumnRenamed(\"START\", \"medication_start\")\n",
    "               .withColumn(\"medication_start\", to_date(\"medication_start\"))\n",
    "               .withColumnRenamed(\"STOP\", \"medication_stop\")\n",
    "               .withColumn(\"medication_stop\", to_date(\"medication_stop\"))\n",
    "               .withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "               .withColumnRenamed(\"PAYER\", \"payer_id\")\n",
    "               .withColumnRenamed(\"ENCOUNTER\", \"encounter_id\")\n",
    "               .withColumnRenamed(\"CODE\", \"medication_code\")\n",
    "               .withColumnRenamed(\"DESCRIPTION\", \"medication_description\")\n",
    ")\n",
    "payer_transitions = (\n",
    "    payer_transitions.withColumnRenamed(\"PATIENT\", \"patient_id\")\n",
    "                     .withColumnRenamed(\"PAYER\", \"payer_id\")\n",
    ")\n",
    "payers = (\n",
    "    payers.withColumnRenamed(\"Id\", \"payer_id\")\n",
    "          .withColumnRenamed(\"NAME\", \"payer_name\")\n",
    "          .withColumnRenamed(\"OWNERSHIP\", \"payer_ownership\")\n",
    ")\n",
    "providers = (\n",
    "    providers.withColumnRenamed(\"Id\", \"provider_id\")\n",
    "             .withColumnRenamed(\"SPECIALITY\", \"provider_specialty\")\n",
    ")\n",
    "organizations = (\n",
    "    organizations.withColumnRenamed(\"Id\", \"organization_id\")\n",
    "                 .withColumnRenamed(\"NAME\", \"organization_name\")\n",
    "                 .withColumnRenamed(\"ZIP\", \"organization_zip\")\n",
    ")\n",
    "organizations = organizations.withColumn(\n",
    "    \"organization_zip\",\n",
    "    col(\"organization_zip\").cast(\"string\")\n",
    ")\n",
    "\n",
    "#adding leading 0's to zip codes to retain their information\n",
    "organizations = organizations.withColumn(\n",
    "    \"organization_zip\",\n",
    "    when(length(col(\"organization_zip\")) == 4,  \n",
    "    lpad(col(\"organization_zip\"), 5, \"0\")\n",
    ").otherwise(col(\"organization_zip\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Merge together dataframes on various id fields that will be used for ML modeling...\n",
    "encounters = (\n",
    "    encounters\n",
    "    .join(payers.select(\"payer_id\", \"payer_name\", \"payer_ownership\"), on=\"payer_id\", how=\"left\")\n",
    "    .join(organizations.select(\"organization_id\", \"organization_name\", \"organization_zip\"), on=\"organization_id\", how=\"left\")\n",
    "    .join(providers.select(\"provider_id\", \"provider_specialty\"), on=\"provider_id\", how=\"left\")\n",
    "    .join(procedures.select(\"encounter_id\", \"procedure_descriptions\", \"procedure_code\"), on=\"encounter_id\", how=\"left\")\n",
    "    .join(patient.select(\"patient_id\", \"BIRTHDATE\", \"patient_marital\", \"patient_race\", \"patient_ethnicity\", \"patient_gender\", \"patient_zip\"), on=\"patient_id\", how=\"left\")\n",
    "    .withColumn(\"age_at_encounter\", floor(datediff(col(\"encounter_start\"), col(\"BIRTHDATE\")) / 365.25))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wq_Bd8YldnMb",
    "outputId": "cc2062fa-0a87-4d51-d6c5-7c916e75cffd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 21:33:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------------+-------------------+----------------+--------------+----------+-----------------+------------+----------+---------------+--------------------+----------------+------------------+----------------------+--------------+----------+---------------+------------+-----------------+--------------+-----------+----------------+\n",
      "|          patient_id|        encounter_id|         provider_id|     organization_id|            payer_id|encounter_start|encounter_stop|ENCOUNTERCLASS|encounter_code|encounter_discription|BASE_ENCOUNTER_COST|TOTAL_CLAIM_COST|PAYER_COVERAGE|REASONCODE|REASONDESCRIPTION|PATIENT COST|payer_name|payer_ownership|   organization_name|organization_zip|provider_specialty|procedure_descriptions|procedure_code| BIRTHDATE|patient_marital|patient_race|patient_ethnicity|patient_gender|patient_zip|age_at_encounter|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------------+-------------------+----------------+--------------+----------+-----------------+------------+----------+---------------+--------------------+----------------+------------------+----------------------+--------------+----------+---------------+------------+-----------------+--------------+-----------+----------------+\n",
      "|ae1119f5-aa83-2a4...|b69cc6e6-6cbe-38d...|79e554fa-6eb7-3f4...|34f0b10a-5e29-31d...|26aab0cd-6aba-3e1...|     2009-08-18|    2009-08-18|      wellness|     162673000| General examinati...|              136.8|           704.2|           0.0|      NULL|             NULL|       704.2|    Humana|        PRIVATE|                NULL|            NULL|              NULL|                  NULL|          NULL|1991-06-25|              M|       white|      nonhispanic|             F|          0|              18|\n",
      "|ae1119f5-aa83-2a4...|b69cc6e6-6cbe-38d...|79e554fa-6eb7-3f4...|34f0b10a-5e29-31d...|26aab0cd-6aba-3e1...|     2009-08-18|    2009-08-18|      wellness|     162673000| General examinati...|              136.8|           704.2|           0.0|      NULL|             NULL|       704.2|    Humana|        PRIVATE|                NULL|            NULL|              NULL|                  NULL|          NULL|1991-06-25|              M|       white|      nonhispanic|             F|          0|              18|\n",
      "|ae1119f5-aa83-2a4...|5019df4c-9990-9e5...|79e554fa-6eb7-3f4...|34f0b10a-5e29-31d...|26aab0cd-6aba-3e1...|     2013-08-27|    2013-08-27|      wellness|     162673000| General examinati...|              136.8|         1186.78|           0.0|      NULL|             NULL|     1186.78|    Humana|        PRIVATE|                NULL|            NULL|              NULL|                  NULL|          NULL|1991-06-25|              M|       white|      nonhispanic|             F|          0|              22|\n",
      "|ae1119f5-aa83-2a4...|5019df4c-9990-9e5...|79e554fa-6eb7-3f4...|34f0b10a-5e29-31d...|26aab0cd-6aba-3e1...|     2013-08-27|    2013-08-27|      wellness|     162673000| General examinati...|              136.8|         1186.78|           0.0|      NULL|             NULL|     1186.78|    Humana|        PRIVATE|                NULL|            NULL|              NULL|                  NULL|          NULL|1991-06-25|              M|       white|      nonhispanic|             F|          0|              22|\n",
      "|ae1119f5-aa83-2a4...|0d2fab60-1be9-36c...|78010622-7176-3cc...|121b0ad1-1beb-3ca...|26aab0cd-6aba-3e1...|     2014-12-30|    2014-12-30|    urgentcare|     702927004| Urgent care clini...|             142.58|          845.98|        845.98|      NULL|             NULL|         0.0|    Humana|        PRIVATE|PATRIOT URGENT CA...|        24517447|              NULL|                  NULL|          NULL|1991-06-25|              M|       white|      nonhispanic|             F|          0|              23|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------------+-------------------+----------------+--------------+----------+-----------------+------------+----------+---------------+--------------------+----------------+------------------+----------------------+--------------+----------+---------------+------------+-----------------+--------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encounters.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2mNaI6flhLXz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient_id',\n",
       " 'encounter_id',\n",
       " 'provider_id',\n",
       " 'organization_id',\n",
       " 'payer_id',\n",
       " 'encounter_start',\n",
       " 'encounter_stop',\n",
       " 'ENCOUNTERCLASS',\n",
       " 'encounter_code',\n",
       " 'encounter_discription',\n",
       " 'BASE_ENCOUNTER_COST',\n",
       " 'TOTAL_CLAIM_COST',\n",
       " 'PAYER_COVERAGE',\n",
       " 'REASONCODE',\n",
       " 'REASONDESCRIPTION',\n",
       " 'PATIENT COST',\n",
       " 'payer_name',\n",
       " 'payer_ownership',\n",
       " 'organization_name',\n",
       " 'organization_zip',\n",
       " 'provider_specialty',\n",
       " 'procedure_descriptions',\n",
       " 'procedure_code',\n",
       " 'BIRTHDATE',\n",
       " 'patient_marital',\n",
       " 'patient_race',\n",
       " 'patient_ethnicity',\n",
       " 'patient_gender',\n",
       " 'patient_zip',\n",
       " 'age_at_encounter']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['patient_id',\n",
       " 'encounter_id',\n",
       " 'provider_id',\n",
       " 'organization_id',\n",
       " 'payer_id',\n",
       " 'encounter_start',\n",
       " 'encounter_stop',\n",
       " 'ENCOUNTERCLASS',\n",
       " 'encounter_code',\n",
       " 'encounter_discription',\n",
       " 'BASE_ENCOUNTER_COST',\n",
       " 'TOTAL_CLAIM_COST',\n",
       " 'PAYER_COVERAGE',\n",
       " 'REASONCODE',\n",
       " 'REASONDESCRIPTION',\n",
       " 'PATIENT COST',\n",
       " 'payer_name',\n",
       " 'payer_ownership',\n",
       " 'organization_name',\n",
       " 'organization_zip',\n",
       " 'provider_specialty',\n",
       " 'procedure_descriptions',\n",
       " 'procedure_code',\n",
       " 'BIRTHDATE',\n",
       " 'patient_marital',\n",
       " 'patient_race',\n",
       " 'patient_ethnicity',\n",
       " 'patient_gender',\n",
       " 'patient_zip',\n",
       " 'age_at_encounter']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encounters.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "modeling_df = encounters.select(\n",
    "    col(\"PATIENT COST\").cast(\"double\").alias(\"label\"),\n",
    "    col(\"age_at_encounter\").cast(\"double\"),\n",
    "    col(\"patient_marital\"),\n",
    "    col(\"patient_race\"),\n",
    "    col(\"patient_ethnicity\"),\n",
    "    col(\"patient_gender\"),\n",
    "    col(\"ENCOUNTERCLASS\"),\n",
    "    col(\"payer_ownership\"),\n",
    "    col(\"payer_name\"),\n",
    "    col(\"organization_zip\"),\n",
    "    col(\"organization_name\"),\n",
    "    col(\"procedure_code\"),\n",
    "    #col(\"encounter_discription\"), # don't use for RF, LR and GBT models\n",
    "    col(\"encounter_code\"),\n",
    "    #col(\"REASONDESCRIPTION\"), # don't use for RF, LR and GBT models\n",
    ").na.drop().filter(col(\"PATIENT COST\") != 0)\n",
    "\n",
    "\n",
    "# Define categorical and numeric columns\n",
    "categorical_cols = ['patient_marital', 'patient_race', 'patient_ethnicity', \n",
    "                   'patient_gender', 'ENCOUNTERCLASS',\n",
    "                   'payer_ownership',\"payer_name\",\"organization_name\", \"organization_zip\", 'procedure_code',\"encounter_code\"]\n",
    "numeric_cols = ['age_at_encounter']\n",
    "\n",
    "target_col = \"label\"\n",
    "\n",
    "# Create feature engineering pipeline stages\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") \n",
    "            for col in categorical_cols]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[col+\"_index\" for col in categorical_cols],\n",
    "    outputCols=[col+\"_encoded\" for col in categorical_cols],\n",
    "    dropLast=True\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols + [col+\"_encoded\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Cache after feature engineering\n",
    "feature_pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "feature_model = feature_pipeline.fit(modeling_df)\n",
    "feature_df = feature_model.transform(modeling_df).select(\"features\", target_col).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45140"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting executed in 0.04 seconds\n",
      "Data splitting executed in 0.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting training and test dataframes nd present the number of partitions of the training dataframe\n",
    "train_df, test_df = time_execution(\n",
    "    \"Data splitting\",\n",
    "    lambda: feature_df.randomSplit([0.8, 0.2], seed=53)\n",
    ")[0]\n",
    "train_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Partitions: 12\n"
     ]
    }
   ],
   "source": [
    "# Repartition data to distribute it across the cluster\n",
    "train_df = train_df.repartition(12)\n",
    "\n",
    "# Optional: Check how many partitions are in the RDD\n",
    "print(f\"Training Data Partitions: {train_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training executed in 19.26 seconds\n",
      "Random Forest Regression Results:\n",
      "RMSE: 1747.8312763425588\n",
      "R2: 0.7661240686501508\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "\n",
    "# Common evaluator for regression tasks\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=30,  # Reduced for initial testing\n",
    "    maxDepth=10,\n",
    "    subsamplingRate=0.7,\n",
    "    featureSubsetStrategy='sqrt',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model, rf_time = time_execution(\n",
    "    \"Random Forest training\",\n",
    "    lambda: rf.fit(train_df)\n",
    ")\n",
    "\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "\n",
    "print(\"Random Forest Regression Results:\")\n",
    "print(\"RMSE:\", reg_evaluator.evaluate(rf_predictions, {reg_evaluator.metricName: \"rmse\"}))\n",
    "print(\"R2:\", reg_evaluator.evaluate(rf_predictions, {reg_evaluator.metricName: \"r2\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|(577,[0,1,5,8,10,...|            159.11|1078.3459026211667|\n",
      "|(577,[0,1,5,8,10,...|            159.11|1078.3459026211667|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002| 992.5397008297834|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995| 940.0779554792844|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 940.0779554792844|\n",
      "|(577,[0,1,5,8,10,...| 480.5000000000001|1265.2417193356916|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002|1095.5674661387266|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|            159.11|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|            159.11|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...|369.60000000000014|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001| 1024.834461075793|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003|1043.1057207882272|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003|1043.1057207882272|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression training executed in 6.97 seconds\n",
      "\n",
      "Linear Regression Results:\n",
      "RMSE: 1803.6438931701152\n",
      "R2: 0.7509491041116775\n"
     ]
    }
   ],
   "source": [
    "### Linear Regression\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "lr_model, lr_time = time_execution(\n",
    "    \"Linear Regression training\",\n",
    "    lambda: lr.fit(train_df)\n",
    ")\n",
    "\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "print(\"\\nLinear Regression Results:\")\n",
    "print(\"RMSE:\", reg_evaluator.evaluate(lr_predictions, {reg_evaluator.metricName: \"rmse\"}))\n",
    "print(\"R2:\", reg_evaluator.evaluate(lr_predictions, {reg_evaluator.metricName: \"r2\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|(577,[0,1,5,8,10,...|            159.11|-705.2915888145076|\n",
      "|(577,[0,1,5,8,10,...|            159.11|-705.2915888145076|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002|-662.1490766850343|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995|-273.8664675197756|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 71.27362951600983|\n",
      "|(577,[0,1,5,8,10,...| 480.5000000000001|  963.179869443896|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002|137.94835713634055|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995| 526.2309663015994|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 828.2285512079118|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 828.2285512079118|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 828.2285512079118|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 871.3710633373851|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 871.3710633373851|\n",
      "|(577,[0,1,5,8,10,...|369.60000000000014| 871.3710633373851|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001| 1251.876835681281|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001|1024.4701200172933|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003|1215.1543306093047|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 804.8326873670264|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 804.8326873670264|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 804.8326873670264|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT training executed in 119.60 seconds\n",
      "\n",
      "GBT Regression Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 850:>                                                        (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 607.5510575748652\n",
      "R2: 0.9717413064781539\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Gradient Boosted Trees Regressor\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=20,  # Reduced iterations\n",
    "    maxDepth=10,\n",
    "    stepSize=0.1,\n",
    "    subsamplingRate=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model, gbt_time = time_execution(\n",
    "    \"GBT training\",\n",
    "    lambda: gbt.fit(train_df)\n",
    ")\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "print(\"\\nGBT Regression Results:\")\n",
    "print(\"RMSE:\", reg_evaluator.evaluate(gbt_predictions, {reg_evaluator.metricName: \"rmse\"}))\n",
    "print(\"R2:\", reg_evaluator.evaluate(gbt_predictions, {reg_evaluator.metricName: \"r2\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|(577,[0,1,5,8,10,...|            159.11| 648.4175865061991|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 648.4175865061991|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002| 648.4175865061991|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 480.5000000000001|422.49699698298963|\n",
      "|(577,[0,1,5,8,10,...| 411.7700000000002| 648.4175865061991|\n",
      "|(577,[0,1,5,8,10,...|185.55999999999995| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|172.90999999999997| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|            159.11| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...|369.60000000000014| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 288.8900000000001| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 234.7496735154522|\n",
      "|(577,[0,1,5,8,10,...| 751.7200000000003| 234.7496735154522|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of ML Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 21:31:23 WARN DAGScheduler: Broadcasting large task binary with size 1208.4 KiB\n",
      "25/05/05 21:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1565.9 KiB\n",
      "25/05/05 21:31:32 WARN DAGScheduler: Broadcasting large task binary with size 2008.1 KiB\n",
      "25/05/05 21:31:37 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Results:\n",
      "RMSE: 998.4577255748717\n",
      "R2: 0.9236786787133897\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.functions import monotonically_increasing_id, expr\n",
    "\n",
    "# Split the training data into 3 parts for the 3 models\n",
    "rf_df, lr_df, gbt_df = train_df.randomSplit([1.0, 1.0, 1.0], seed=42)\n",
    "\n",
    "# Define model training functions\n",
    "def train_rf():\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=30, maxDepth=10, seed=42)\n",
    "    return rf.fit(rf_df)\n",
    "\n",
    "def train_lr():\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", regParam=0.01, elasticNetParam=0.5)\n",
    "    return lr.fit(lr_df)\n",
    "\n",
    "def train_gbt():\n",
    "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxIter=10, maxDepth=5, stepSize=0.1, seed=42)\n",
    "    return gbt.fit(gbt_df)\n",
    "\n",
    "# Train models in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    rf_future = executor.submit(train_rf)\n",
    "    lr_future = executor.submit(train_lr)\n",
    "    gbt_future = executor.submit(train_gbt)\n",
    "\n",
    "    rf_model = rf_future.result()\n",
    "    lr_model = lr_future.result()\n",
    "    gbt_model = gbt_future.result()\n",
    "\n",
    "# Make predictions and add row indices\n",
    "rf_pred = rf_model.transform(test_df).select(\"prediction\", \"label\") \\\n",
    "    .withColumnRenamed(\"prediction\", \"rf_pred\") \\\n",
    "    .withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "\n",
    "lr_pred = lr_model.transform(test_df).select(\"prediction\") \\\n",
    "    .withColumnRenamed(\"prediction\", \"lr_pred\") \\\n",
    "    .withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "\n",
    "gbt_pred = gbt_model.transform(test_df).select(\"prediction\") \\\n",
    "    .withColumnRenamed(\"prediction\", \"gbt_pred\") \\\n",
    "    .withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "\n",
    "# Join predictions on row index\n",
    "ensemble_df = rf_pred \\\n",
    "    .join(lr_pred, on=\"row_idx\") \\\n",
    "    .join(gbt_pred, on=\"row_idx\")\n",
    "\n",
    "# Compute average prediction\n",
    "ensemble_df = ensemble_df.withColumn(\"ensemble_prediction\", expr(\"(rf_pred + lr_pred + gbt_pred)/3\"))\n",
    "\n",
    "# Evaluate ensemble\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"ensemble_prediction\")\n",
    "\n",
    "print(\"\\nEnsemble Results:\")\n",
    "print(\"RMSE:\", reg_evaluator.evaluate(ensemble_df, {reg_evaluator.metricName: \"rmse\"}))\n",
    "print(\"R2:\", reg_evaluator.evaluate(ensemble_df, {reg_evaluator.metricName: \"r2\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------------+-----------------+-------------------+\n",
      "|row_idx|          rf_pred|             label|            lr_pred|         gbt_pred|ensemble_prediction|\n",
      "+-------+-----------------+------------------+-------------------+-----------------+-------------------+\n",
      "|      0|704.9655962776457|            159.11| -606.4865431954435|401.1089130420247| 166.52932204140896|\n",
      "|      1|704.9655962776457|            159.11| -606.4865431954435|401.1089130420247| 166.52932204140896|\n",
      "|      2|704.9655962776457| 411.7700000000002| -564.6929316237091|401.1089130420247| 180.46052589865374|\n",
      "|      3|620.3533124835761|185.55999999999995|-188.55042747810228|401.1089130420247| 277.63726601583284|\n",
      "|      4|683.7251174627415|            159.11| 145.79846509577067|401.1089130420247| 410.21083186684564|\n",
      "|      5|620.3533124835761| 480.5000000000001| 1331.6097492300871|401.1089130420247|  784.3573249185625|\n",
      "|      6|704.9655962776457| 411.7700000000002|  798.0738127022855|401.1089130420247|  634.7161073406519|\n",
      "|      7|620.3533124835761|185.55999999999995| 1174.2163168478921|401.1089130420247|   731.892847457831|\n",
      "|      8|683.7251174627415|172.90999999999997| 1466.7715978500303|401.1089130420247|  850.5352094515988|\n",
      "|      9|683.7251174627415|172.90999999999997| 1466.7715978500303|401.1089130420247|  850.5352094515988|\n",
      "|     10|683.7251174627415|172.90999999999997| 1466.7715978500303|401.1089130420247|  850.5352094515988|\n",
      "|     11|683.7251174627415|            159.11|  1508.565209421765|401.1089130420247|  864.4664133088437|\n",
      "|     12|683.7251174627415|            159.11|  1508.565209421765|401.1089130420247|  864.4664133088437|\n",
      "|     13|683.7251174627415|369.60000000000014|  1508.565209421765|401.1089130420247|  864.4664133088437|\n",
      "|     14|683.7251174627415| 288.8900000000001|  615.5429543853318|401.1089130420247|  566.7923282966993|\n",
      "|     15|683.7251174627415| 288.8900000000001|  699.2934196845692|401.1089130420247|  594.7091500631118|\n",
      "|     16|683.7251174627415| 751.7200000000003| 1659.6042915615408|401.1089130420247|  914.8127740221024|\n",
      "|     17|683.7251174627415| 751.7200000000003|  757.4784918417138|401.1089130420247|  614.1041741154933|\n",
      "|     18|683.7251174627415| 751.7200000000003|  757.4784918417138|401.1089130420247|  614.1041741154933|\n",
      "|     19|683.7251174627415| 751.7200000000003|  757.4784918417138|401.1089130420247|  614.1041741154933|\n",
      "+-------+-----------------+------------------+-------------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_df.show(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
